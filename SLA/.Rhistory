rm(list=ls())
gc()
.rs.restartR()
devtools::load_all()
rm(list=ls())
gc()
.rs.restartR()
devtools::load_all()
fileName = '../dataset/ticket_cientista.csv'
preProcessedData = dataInspect(fileName = fileName)
hist(as.POSIXct(preProcessedData$closeDateTime), "days", format = "%d %b")
devtools::load_all()
fileName = '../dataset/ticket_cientista.csv'
preProcessedData = dataInspect(fileName = fileName)
hist(as.POSIXct(preProcessedData$closeDateTime), "days", format = "%d %b")
rm(list=ls())
gc()
.rs.restartR()
devtools::load_all()
fileName = '../dataset/ticket_cientista.csv'
preProcessedData = defData(fileName = fileName)
rm(list=ls())
gc()
.rs.restartR()
devtools::load_all()
fileName = '../dataset/ticket_cientista.csv'
preProcessedData = defData(dataset_filename = fileName)
preProcessedData
View(preProcessedData)
rm(list=ls())
gc()
.rs.restartR()
devtools::load_all()
fileName = '../dataset/ticket_cientista.csv'
dataset = defData(dataset_filename = fileName)
customers = customerCodes(preProcessedData)
rm(list=ls())
gc()
.rs.restartR()
devtools::load_all()
fileName = '../dataset/ticket_cientista.csv'
dataset = defData(dataset_filename = fileName)
customers = customerCodes(preProcessedData)
customers = customerCodes(dataset)
customers
rm(list=ls())
gc()
.rs.restartR()
devtools::load_all()
fileName = '../dataset/ticket_cientista.csv'
dataset = defData(dataset_filename = fileName)
customers = customerCodes(dataset)
preProcessedData = harmonizeDateTime(dataset)
harmonizeDateTime(preProcessedData)
hist(as.POSIXct(preProcessedData$closeDateTime), "days", format = "%d %b")
hist(as.POSIXct(dataset$closeDateTime), "days", format = "%d %b")
dataset$closeDateTime
preProcessedData
histogramDateTime(preProcessedData)
preProcessedData$closeDateTime
rm(list=ls())
gc()
.rs.restartR()
devtools::load_all()
devtools::load_all()
fileName = '../dataset/ticket_cientista.csv'
dataset = defData(dataset_filename = fileName)
customers = customerCodes(dataset)
preProcessedData = harmonizeDateTime(dataset)
histogramDateTime(preProcessedData)
hist(as.POSIXct(preProcessedData$closeDateTime), "days", format = "%d %b")
preProcessedData$closeDateTime
rm(list=ls())
gc()
.rs.restartR()
devtools::load_all()
fileName = '../dataset/ticket_cientista.csv'
dataset = defData(dataset_filename = fileName)
customers = customerCodes(dataset)
customers
preProcessedData = harmonizeDateTime(dataset)
View(preProcessedData)
histogramDateTime(preProcessedData)
?hist(as.POSIXct(dataset$closeDateTime), "days", format = "%d %b")
hist(as.POSIXct(dataset$closeDateTime), "days", format = "%d %b")
hist(as.POSIXct(preProcessedData$closeDateTime), "days", format = "%d %b")
hist(as.POSIXct(dataset$closeDateTime), "days", format = "%d %b")
preProcessedData
preProcessedData = preProcessedData[order(preProcessedData$closeDateTime),]
View(preProcessedData)
dataset$closeDateTime
View(dataset)
?sleep
rm(list=ls())
gc()
.rs.restartR()
Sys.sleep(1)
rm(list=ls())
gc()
devtools::load_all()
fileName = '../dataset/ticket_cientista.csv'
dataset = defData(dataset_filename = fileName)
customers = customerCodes(dataset)
preProcessedData = harmonizeDateTime(dataset)
histogramDateTime(preProcessedData)
View(dataset)
View(dataset)
# Only not null close date time entries
dataset = dataset[dataset$closeDateTime != 'null',]
# Converting date time format
charDateTime = gsub(x = gsub(x = as.character(dataset$closeDateTime), pattern = 'ISODate', replacement = '') , replacement = '', pattern = "\"")
charDateTime = gsub(x = charDateTime, pattern = "\\(", replacement = '')
charDateTime = gsub(x = charDateTime, pattern = "\\)", replacement = '')
charDateTime = gsub(x = charDateTime, pattern = "T", replacement = ' ')
# Slicing the strings of closeDateTime
charDateTime = sapply(charDateTime, function(x){substr(x = x, start = 1, stop = 19)})
charDateTime
# removes the entry solved from 2017
dataset = dataset[order(dataset$closeDateTime),]
View(dataset)
dataset = dataset[-c(1),]
# Variables names
#vars = colnames(dataset)
# SLA status
#slaStatus = unique(dataset$slaStatus)
# Call status
#callStatus = unique(dataset$callStatus)
# Was solved on time?
#onTimeStatus = unique(dataset$onTimeSolution)
# how many?
#nOnTime = length(dataset$onTimeSolution[dataset$onTimeSolution == 'S'])
# what is the percentage of total of calls?
#pOnTime = nOnTime/length(dataset$onTimeSolution)
# How is the time distribution of solved calls? The solved calls are defined below
#solvedCalls  = dataset[dataset$callStatus %in% c('N0', 'N4', 'CV'),]
# Only not null close date time entries
#notNullDate = dataset[dataset$closeDateTime != 'null',]
# Solved calls of notNullDate
#notNullSolved = notNullDate[notNullDate$callStatus %in%c('N0', 'N4', 'CV'),]
# Only not null close date time entries
dataset = dataset[dataset$closeDateTime != 'null',]
# Converting date time format
charDateTime = gsub(x = gsub(x = as.character(dataset$closeDateTime), pattern = 'ISODate', replacement = '') , replacement = '', pattern = "\"")
charDateTime = gsub(x = charDateTime, pattern = "\\(", replacement = '')
charDateTime = gsub(x = charDateTime, pattern = "\\)", replacement = '')
charDateTime = gsub(x = charDateTime, pattern = "T", replacement = ' ')
# Slicing the strings of closeDateTime
charDateTime = sapply(charDateTime, function(x){substr(x = x, start = 1, stop = 19)})
# returns the harmonized date/time set to original dataset
dataset$closeDateTime = charDateTime
# removes the entry solved from 2017
dataset = dataset[order(dataset$closeDateTime),]
head(dataset)
dataset = defData(dataset_filename = fileName)
customers = customerCodes(dataset)
# Variables names
#vars = colnames(dataset)
# SLA status
#slaStatus = unique(dataset$slaStatus)
# Call status
#callStatus = unique(dataset$callStatus)
# Was solved on time?
#onTimeStatus = unique(dataset$onTimeSolution)
# how many?
#nOnTime = length(dataset$onTimeSolution[dataset$onTimeSolution == 'S'])
# what is the percentage of total of calls?
#pOnTime = nOnTime/length(dataset$onTimeSolution)
# How is the time distribution of solved calls? The solved calls are defined below
#solvedCalls  = dataset[dataset$callStatus %in% c('N0', 'N4', 'CV'),]
# Only not null close date time entries
#notNullDate = dataset[dataset$closeDateTime != 'null',]
# Solved calls of notNullDate
#notNullSolved = notNullDate[notNullDate$callStatus %in%c('N0', 'N4', 'CV'),]
# Only not null close date time entries
dataset = dataset[dataset$closeDateTime != 'null',]
# Converting date time format
charDateTime = gsub(x = gsub(x = as.character(dataset$closeDateTime), pattern = 'ISODate', replacement = '') , replacement = '', pattern = "\"")
charDateTime = gsub(x = charDateTime, pattern = "\\(", replacement = '')
charDateTime = gsub(x = charDateTime, pattern = "\\)", replacement = '')
charDateTime = gsub(x = charDateTime, pattern = "T", replacement = ' ')
# Slicing the strings of closeDateTime
charDateTime = sapply(charDateTime, function(x){substr(x = x, start = 1, stop = 19)})
# returns the harmonized date/time set to original dataset
dataset$closeDateTime = charDateTime
head(dataset)
# removes the entry solved from 2017
dataset = dataset[order(dataset$closeDateTime),]
head(dataset)
dataset = dataset[-c(1),]
head(dataset)
rm(list=ls())
gc()
devtools::load_all()
fileName = '../dataset/ticket_cientista.csv'
dataset = defData(dataset_filename = fileName)
customers = customerCodes(dataset)
preProcessedData = harmonizeDateTime(dataset)
histogramDateTime(preProcessedData)
?stop()
devtools::load_all()
SLA()
devtools::load_all()]
devtools::load_all()
SLA(config_json_filename = "teste.json")
install.packages('jsonlite')
devtools::load_all()
devtools::document()
devtools::load_all()
devtools::document()
devtools::load_all()
devtools::load_all()
devtools::document()
devtools::load_all()
cfg = fromJSON('../SLA/SLA_config.json')
cfg
cfg$input
cfg = fromJSON('../SLA/SLA_config.json')
cfg$input
cfg$input$filename
cfg$input$exist_header
cfg$input$separator
cfg = fromJSON('../SLA/SLA_config.json')
cfg = fromJSON('../SLA/SLA_config.json')
exists(cfg$input$filename)
cfg$input$filename
cfg = fromJSON('../SLA/SLA_config.json')
cfg$input$filename
exists(cfg$input$filename)
file]exists(cfg$input$filename)
file.exists(cfg$input$filename)
read.csv(cfg$input$filename)
cfg = fromJSON('../SLA/SLA_config.json')
exists(cfg$input$filename)
file.exists(cfg$input$filename)
dataset[cfg$pre_process$callNumber_col]
rm(list=ls())
gc()
devtools::load_all()
rm(list=ls())
gc()
devtools::load_all()
devtools::load_all()
devtools::document()
devtools::load_all()
devtools::document()
devtools::document()
rm(list=ls())
gc()
devtools::load_all()
rm(list=ls())
gc()
devtools::load_all()
warnings()
rm(list=ls())
gc()
devtools::load_all()
config_json_filename = 'SLA_config.json'
# Reading the json config
cfg = fromJSON(config_json_filename)
# Setup folder
dirs = setup_folder(cfg = cfg)
# Read data
dataset = read.csv(file = paste0(cfg$folders$input_folder, cfg$pre_process$filename),
header = cfg$pre_process$exist_header,
sep = cfg$pre_process$separator)
relevant_cols = c(cfg$pre_process$closed_ticket_col,
cfg$pre_process$closed_ontime_col,
cfg$pre_process$closeDate_col,
cfg$pre_process$slaStatus_col,
cfg$pre_process$customers_col,
cfg$pre_process$callNumber_col)
dataset = dataset[, names(dataset) %in% c(relevant_cols)]
# Only not null close date time entries
colTime = cfg$pre_process$closeDate_col
dataset = dataset[dataset[colTime] != 'null',]
# Converting date time format
charDateTime = as.character(dataset[,colTime])
charDateTime = gsub(x = charDateTime, pattern = 'ISODate', replacement = '')
charDateTime = gsub(x = charDateTime, pattern = "\"", replacement = '')
charDateTime = gsub(x = charDateTime, pattern = "\\(", replacement = '')
charDateTime = gsub(x = charDateTime, pattern = "\\)", replacement = '')
charDateTime = gsub(x = charDateTime, pattern = "T", replacement = ' ')
# Slicing the strings of closeDateTime
charDateTime = sapply(charDateTime, function(x){substr(x = x, start = 1, stop = 19)})
dataset[cfg$pre_process$callNumber_col]
length(dataset[cfg$pre_process$callNumber_col])
length(unlist(dataset[cfg$pre_process$callNumber_col]))
length(charDateTime)
unlist(dataset[cfg$pre_process$callNumber_col])
as.character(unlist(dataset[cfg$pre_process$callNumber_col]))
names(charDateTime) = as.character(unlist(dataset[cfg$pre_process$callNumber_col]))
charDateTime
names(charDateTime)
# Parallelism setup
threads = cfg$pre_process$threads
cl <- parallel::makeCluster(threads, type = 'SOCK', outfile = "")
doParallel::registerDoParallel(cl)
on.exit(stopCluster(cl))
foreach (iter_name = names(charDateTime)[1:length(charDateTime)]) %dopar% {
print(iter_name)
}
# Entries in the correct time range
corTimeRange = rep(0, length(charDateTime))
corTimeRange
names(corTimeRange) = names(charDateTime)
corTimeRange
charDateTime['17211380']
charDateTime
iter_name = names(charDateTime)[1]
iter_name
corTimeRange = as.list(NA)
corTimeRange
iter_name = as.character(iter_name)
names(corTimeRange) = iter_name
corTimeRange[[iter_name]]
charDateTime[iter_name]
charDateTime[[iter_name]]
rm(list=ls())
gc()
devtools::load_all()
config_json_filename = 'SLA_config.json'
warnings()
devtools::document()
devtools::load_all()
devtools::document()
devtools::load_all()
rm(list=ls())
gc()
devtools::load_all()
config_json_filename = 'SLA_config.json'
# Reading the json config
cfg = fromJSON(config_json_filename)
# Setup folder
dirs = setup_folder(cfg = cfg)
# Read data
dataset = read.csv(file = paste0(cfg$folders$input_folder, cfg$pre_process$filename),
header = cfg$pre_process$exist_header,
sep = cfg$pre_process$separator)
relevant_cols = c(cfg$pre_process$closed_ticket_col,
cfg$pre_process$closed_ontime_col,
cfg$pre_process$closeDate_col,
cfg$pre_process$slaStatus_col,
cfg$pre_process$customers_col,
cfg$pre_process$callNumber_col)
dataset = dataset[, names(dataset) %in% c(relevant_cols)]
# Only not null close date time entries
colTime = cfg$pre_process$closeDate_col
dataset = dataset[dataset[colTime] != 'null',]
# Converting date time format
charDateTime = as.character(dataset[,colTime])
charDateTime = gsub(x = charDateTime, pattern = 'ISODate', replacement = '')
charDateTime = gsub(x = charDateTime, pattern = "\"", replacement = '')
charDateTime = gsub(x = charDateTime, pattern = "\\(", replacement = '')
charDateTime = gsub(x = charDateTime, pattern = "\\)", replacement = '')
charDateTime = gsub(x = charDateTime, pattern = "T", replacement = ' ')
# Slicing the strings of closeDateTime
charDateTime = sapply(charDateTime, function(x){substr(x = x, start = 1, stop = 19)})
names(charDateTime) = as.character(unlist(dataset[cfg$pre_process$callNumber_col]))
# Ensure only entries from the correct time range are considered
iniDate = cfg$pre_process$initial_date
endDate = cfg$pre_process$end_date
# Parallelism setup
threads = cfg$pre_process$threads
cl <- parallel::makeCluster(threads, type = 'SOCK', outfile = "")
doParallel::registerDoParallel(cl)
on.exit(stopCluster(cl))
# Entries in the correct time range
corTimeRange = foreach (iter_name = names(charDateTime)[1:length(charDateTime)]) %dopar% {
corTimeRange = as.list(NA)
iter_name = as.character(iter_name)
names(corTimeRange) = iter_name
corTimeRange[[iter_name]] = charDateTime[[iter_name]]
return(corTimeRange)
}
corTimeRange
unlist(corTimeRange)
length(unlist(corTimeRange))
rm(list=ls())
gc()
devtools::load_all()
devtools::document()
devtools::load_all()
rm(list=ls())
gc()
devtools::load_all()
config_json_filename = 'SLA_config.json'
# Reading the json config
cfg = fromJSON(config_json_filename)
# Setup folder
dirs = setup_folder(cfg = cfg)
# Read data
dataset = read.csv(file = paste0(cfg$folders$input_folder, cfg$pre_process$filename),
header = cfg$pre_process$exist_header,
sep = cfg$pre_process$separator)
relevant_cols = c(cfg$pre_process$closed_ticket_col,
cfg$pre_process$closed_ontime_col,
cfg$pre_process$closeDate_col,
cfg$pre_process$slaStatus_col,
cfg$pre_process$customers_col,
cfg$pre_process$callNumber_col)
dataset = dataset[, names(dataset) %in% c(relevant_cols)]
# Only not null close date time entries
colTime = cfg$pre_process$closeDate_col
dataset = dataset[dataset[colTime] != 'null',]
# Converting date time format
charDateTime = as.character(dataset[,colTime])
charDateTime = gsub(x = charDateTime, pattern = 'ISODate', replacement = '')
charDateTime = gsub(x = charDateTime, pattern = "\"", replacement = '')
charDateTime = gsub(x = charDateTime, pattern = "\\(", replacement = '')
charDateTime = gsub(x = charDateTime, pattern = "\\)", replacement = '')
charDateTime = gsub(x = charDateTime, pattern = "T", replacement = ' ')
# Slicing the strings of closeDateTime
charDateTime = sapply(charDateTime, function(x){substr(x = x, start = 1, stop = 19)})
names(charDateTime) = as.character(unlist(dataset[cfg$pre_process$callNumber_col]))
# Ensure only entries from the correct time range are considered
iniDate = cfg$pre_process$initial_date
endDate = cfg$pre_process$end_date
# Parallelism setup
threads = cfg$pre_process$threads
cl <- parallel::makeCluster(threads, type = 'SOCK', outfile = "")
doParallel::registerDoParallel(cl)
on.exit(stopCluster(cl))
charDateTime[["17227482"]]
charDateTime[["17227482"]] < iniDate
charDateTime[["17227482"]] > iniDate
# Entries in the correct time range
corTimeRange = foreach (iter_name = names(charDateTime)[1:length(charDateTime)]) %dopar% {
corTimeRange = as.list(NA)
iter_name = as.character(iter_name)
names(corTimeRange) = iter_name
if (charDateTime[[iter_name]] >= iniDate & charDateTime[[iter_name]] <= endDate){
corTimeRange[[iter_name]] = charDateTime[[iter_name]]
}
return(corTimeRange)
}
unlist(corTimeRange)
length(which(is.na(unlist(corTimeRange))))
unlist(corTimeRange)[which(is.na(unlist(corTimeRange)))]
charDateTime['14971665']
rm(list=ls())
gc()
devtools::load_all()
config_json_filename = 'SLA_config.json'
SLA(config_json_filename = config_json_filename)
rm(list=ls())
gc()
devtools::load_all()
config_json_filename = 'SLA_config.json'
rm(list=ls())
gc()
devtools::load_all()
config_json_filename = 'SLA_config.json'
# Reading the json config
cfg = fromJSON(config_json_filename)
# Setup folder
dirs = setup_folder(cfg = cfg)
# Check the need to run preprocessing and processing
if (!cfg$post_process$only_postprocess){
# Preprocessing module
if (cfg$pre_process$run_preprocess){
# Read data
dataset = read.csv(file = paste0(cfg$folders$input_folder, cfg$pre_process$filename),
header = cfg$pre_process$exist_header,
sep = cfg$pre_process$separator)
relevant_cols = c(cfg$pre_process$closed_ticket_col,
cfg$pre_process$closed_ontime_col,
cfg$pre_process$closeDate_col,
cfg$pre_process$slaStatus_col,
cfg$pre_process$customers_col,
cfg$pre_process$callNumber_col)
dataset = dataset[, names(dataset) %in% c(relevant_cols)]
# Formatting data correctly if needed
if (cfg$pre_process$format_date){
dataset = convertDate(dataset = dataset, cfg = cfg)
}
# Saving the resultant preprocessed dataset
write.csv(x = dataset, file = paste0(cfg$folders$preprocessed, 'preprocessed_', cfg$pre_process$filename))
} else {
dataset = read.csv(file = paste0(cfg$folders$preprocessed, 'preprocessed_', cfg$pre_process$filename),
header = cfg$pre_process$exist_header,
sep = cfg$pre_process$separator)
}
}
dataset
rm(list=ls())
gc()
devtools::load_all()
config_json_filename = 'SLA_config.json'
SLA(config_json_filename)
rm(list=ls())
gc()
# n = Month sum of number of tickets closed on time (onTimeSolution = S; callStatus = N0, N4 or CV) per day
n_jan = rep(0, 1:31)
# n = Month sum of number of tickets closed on time (onTimeSolution = S; callStatus = N0, N4 or CV) per day
n_jan = ?rep(0, 1:31)
?rep
# n = Month sum of number of tickets closed on time (onTimeSolution = S; callStatus = N0, N4 or CV) per day
n_jan = rep(0, 31)
n_jan
rm(list=ls())
gc()
devtools::load_all()
config_json_filename = 'SLA_config.json'
# Reading the json config
cfg = fromJSON(config_json_filename)
# Setup folder
dirs = setup_folder(cfg = cfg)
cfg$pre_process$run_preprocess
dataset = read.csv(file = paste0(cfg$folders$preprocessed, 'preprocessed_', cfg$pre_process$filename),
header = cfg$pre_process$exist_header,
sep = cfg$pre_process$separator)
dataset
